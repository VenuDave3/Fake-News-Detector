{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f1a542",
   "metadata": {},
   "source": [
    "# N-gram Comparison: Fake vs Real News\n",
    "\n",
    "This short notebook performs a simple, reproducible analysis comparing the most frequent terms\n",
    "(unigrams) in the `Fake.csv` and `True.csv` datasets included in `data/`. The visualization\n",
    "below helps highlight differences in vocabulary between the two datasets — useful as a\n",
    "quick EDA item to show original analysis in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf697160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets (these CSVs are included in `data/`)\n",
    "fake = pd.read_csv('data/Fake.csv')\n",
    "real = pd.read_csv('data/True.csv')\n",
    "# Ensure we have text columns as strings\n",
    "fake_texts = fake['text'].astype(str).tolist()\n",
    "real_texts = real['text'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de24c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_ngrams(corpus, n=15, ngram_range=(1,1)):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "    X = vec.fit_transform(corpus)\n",
    "    sums = X.sum(axis=0)\n",
    "    terms = vec.get_feature_names_out()\n",
    "    freqs = [(terms[i], int(sums[0, i])) for i in range(len(terms))]\n",
    "    return sorted(freqs, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "top_fake = top_ngrams(fake_texts, n=15, ngram_range=(1,1))\n",
    "top_real = top_ngrams(real_texts, n=15, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa8ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for plotting\n",
    "df_fake = pd.DataFrame(top_fake, columns=['term','count'])\n",
    "df_real = pd.DataFrame(top_real, columns=['term','count'])\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,6))\n",
    "sns.barplot(x='count', y='term', data=df_fake, ax=axes[0], palette='Reds_d')\n",
    "axes[0].set_title('Top terms — Fake')\n",
    "sns.barplot(x='count', y='term', data=df_real, ax=axes[1], palette='Blues_d')\n",
    "axes[1].set_title('Top terms — Real')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9f8be",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- This is a compact reproducible analysis: it uses only the CSVs in `data/` and standard libraries.\n",
    "- You can extend this to bi-grams or compare relative frequencies (normalize by document counts)\n",
    "  to surface discriminative phrases between fake and real news."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
